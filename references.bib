@unpublished{cockettReverseDerivativeCategories2019,
  title = {Reverse Derivative Categories},
  author = {Cockett, Robin and Cruttwell, Geoffrey and Gallagher, Jonathan and Lemay, Jean-Simon Pacaud and MacAdam, Benjamin and Plotkin, Gordon and Pronk, Dorette},
  date = {2019-10-15},
  eprint = {1910.07065},
  eprinttype = {arXiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1910.07065},
  urldate = {2024-07-10},
  abstract = {The reverse derivative is a fundamental operation in machine learning and automatic differentiation [1, 11]. This paper gives a direct axiomatization of a category with a reverse derivative operation, in a similar style to that given by [2] for a forward derivative. Intriguingly, a category with a reverse derivative also has a forward derivative, but the converse is not true. In fact, we show explicitly what a forward derivative is missing: a reverse derivative is equivalent to a forward derivative with a dagger structure on its subcategory of linear maps. Furthermore, we show that these linear maps form an additively enriched category with dagger biproducts.},
  langid = {english},
  keywords = {Computer Science - Logic in Computer Science,D.3.1,F.3.2,Mathematics - Category Theory},
  file = {/home/frc/Zotero/storage/FILS3YV2/Cockett et al. - 2019 - Reverse derivative categories.pdf}
}

@article{cruttwellDeepLearningParametric,
  title = {Deep {{Learning}} with {{Parametric Lenses}}},
  author = {Cruttwell, Geoffrey S H and Gavranovic, Bruno and Ghani, Neil and Wilson, Paul and Zanasi, Fabio},
  abstract = {We propose a categorical semantics for machine learning algorithms in terms of lenses, parametric maps, and reverse derivative categories. This foundation provides a powerful explanatory and unifying framework: it encompasses a variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov momentum, as well as a variety of loss functions such as MSE and Softmax cross-entropy, and different architectures, shedding new light on their similarities and differences. Furthermore, our approach to learning has examples generalising beyond the familiar continuous domains (modelled in categories of smooth maps) and can be realised in the discrete setting of Boolean and polynomial circuits. We demonstrate the practical significance of our framework with an implementation in Python.},
  langid = {english},
  file = {/home/frc/Zotero/storage/VIM2JFIN/Cruttwell et al. - Deep Learning with Parametric Lenses.pdf}
}

@unpublished{shieblerCategoryTheoryMachine2021,
  title = {Category {{Theory}} in {{Machine Learning}}},
  author = {Shiebler, Dan and GavranoviÄ‡, Bruno and Wilson, Paul},
  date = {2021-06-13},
  eprint = {2106.07032},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.07032},
  urldate = {2024-07-10},
  abstract = {Over the past two decades machine learning has permeated almost every realm of technology. At the same time, many researchers have begun using category theory as a unifying language, facilitating communication between different scientific disciplines. It is therefore unsurprising that there is a burgeoning interest in applying category theory to machine learning. We aim to document the motivations, goals and common themes across these applications. We touch on gradient-based learning, probability, and equivariant learning.},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/frc/Zotero/storage/3QPN9TYT/Shiebler et al. - 2021 - Category Theory in Machine Learning.pdf}
}
